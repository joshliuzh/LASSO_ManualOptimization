---
title: "Manual Lasso"
author: "Josh Liu"
date: "September 21, 2017"
output:
  pdf_document: default
  html_document: default
  word_document: default
classoption: portrait
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
options(width = 1000)
```

```{r}
btc <- read.csv("bitcoin_dataset.csv")
colnames(btc) <- c("Date","mktPrice","total","mktCap","tradeVol","blkSize","avgBlkSize","orphanedBlk","transPerBlk","medianConfTime","hashRate","dfcty","minerRev","transFee","costPerTransPct","costPerTrans","uniqueAdd","numTrans","transTotal","transExcdPplar","trans_exc_chains_longerthan_100","outputVol","estTransVol","estTransVolUsed")
```

# Question 1


```{r}
plot(btc$mktPrice , xlab = "Index, All data", type = "l", lwd = 3, ylab = "Market Price", col = "dodgerblue") # initial inspection of data

```

After inspecting the data, I take a closer look at the `price` variable:

```{r}
par(mfrow = c(2,2))

plot(y = btc$mktPrice[1: 400] , x = 1:400, xlab = "Index", type = "l", lwd = 3, ylab = "Market Price", col = "dodgerblue") # initial inspection of data

plot(y = btc$mktPrice[401: 800] , x = 401:800, xlab = "Index", type = "l", lwd = 3, ylab = "Market Price", col = "dodgerblue") # initial inspection of data

plot(y = btc$mktPrice[801: 1200]  , x = 801:1200, xlab = "Index", type = "l", lwd = 3, ylab = "Market Price", col = "dodgerblue") # initial inspection of data


plot(y = btc$mktPrice[1201: 1590] , x = 1201:1590, xlab = "Index", type = "l", lwd = 3, ylab = "Market Price", col = "dodgerblue") # initial inspection of data
par(mfrow = c(1, 1))
```

The chart shows that prior to around Index 300, the market price of bitcoin remained identically at 0. My first thought is that I will not use observations indexed less than 300. However, since Professor Zhu said the training dataset should incluce `ALL` observations before 1/1/2017, I will keep them.

I will truncate data after 1/1/2017 in our training dataset.

In addition, observations with any variable being `NA` are removed.

```{r}
b <- which(btc$Date == "2017-01-01 00:00:00")
btc_truncated <- btc[1: b, ]

# drop the variable of tradeVol, as instructed:
btc_truncated <- subset(btc_truncated, select = names(btc_truncated) != "tradeVol")
btc_training <- na.omit(btc_truncated)

btc_testing <- btc[b + 1: length(btc), ]

```

## a)


```{r}
library(leaps)

library(knitr)


bestsubset <- leaps(x = btc_training[, -c(1, 2, 3)], 
      y = btc_training[, "mktPrice"], 
      int = TRUE, 
      method = c("Cp", "bic"), 
      nbest = 1,
      names = colnames(btc_training[, -c(1, 2, 3)]), df=NROW(x))

bestsubset$which


```

For best model with size 1: predictor is `btc_market_cap`;
Size 2: `mktCap` and `difficulty`;
Size 3: `mktCap`, `hashRate` and `minersRevenue`;
Size 4: `mktCap`, `hashrate`, `costPerTrans` and `minersRevenue`;
Size 5: `mktCap`, `hashRate`, `transTotal`, `blockSize`, and `minersRevenue`.

## b)

In this part, I will use a package called `bestglm` to fit the best subset based on AIC and BIC.

In order to use this package, the dataset needs to be further cleaned, leaving no extraneous variables there.

I will drop the variable `Date`. In addition, the reponse variable `mktPrice` will be renamed as `y`.

```{r}

bestCPmodel <- which.min(bestsubset$Cp)
bestsubset$which[13,]

btc.training.for.bestglm <- subset(btc_training, select = names(btc_training) != "Date")
colnames(btc.training.for.bestglm)[1] <- "y"

bestCPmodel <- lm(y ~ mktCap + blkSize + avgBlkSize + orphanedBlk + medianConfTime + hashRate + dfcty + minerRev + transFee + costPerTrans + numTrans + transTotal + transExcdPplar, data = btc.training.for.bestglm)


library(bestglm)



bestmodel.aic <-
    bestglm(Xy = btc.training.for.bestglm,
            family = gaussian,
            IC = "AIC",                 
            method = "exhaustive")
kable(ifelse(bestmodel.aic$BestModels[1,]== TRUE, "*", ""))


bestAICmodel <- lm(y ~ total + blkSize + avgBlkSize + medianConfTime + hashRate + dfcty + minerRev + transFee + costPerTrans +uniqueAdd +numTrans +transTotal+ transExcdPplar + trans_exc_chains_longerthan_100 + outputVol + estTransVol, data = btc.training.for.bestglm)



bestmodel.bic <-
    bestglm(Xy = btc.training.for.bestglm,
            family = gaussian,
            IC = "BIC",                 
            method = "exhaustive")
kable(ifelse(bestmodel.bic$BestModels[1,]== TRUE, "*", ""))

bestBICmodel <- lm(y ~ blkSize + minerRev + transFee + costPerTrans + uniqueAdd + transTotal + transExcdPplar + trans_exc_chains_longerthan_100 + estTransVol, data = btc.training.for.bestglm)

predictedCP <- predict(bestCPmodel, btc_testing)
predictedAIC <- predict(bestAICmodel, btc_testing)
predictedBIC <- predict(bestBICmodel, btc_testing)

predErrCP <- mean((predictedCP - btc_testing$mktPrice) ^ 2)
predErrAIC <- mean((predictedAIC - btc_testing$mktPrice) ^ 2)
predErrBIC <- mean((predictedBIC - btc_testing$mktPrice) ^ 2)

result2b <- as.matrix(c(predErrCP, predErrAIC, predErrBIC))
rownames(result2b) <- c("CP", "AIC", "BIC")
colnames(result2b) <- "Prediction Error"
kable(result2b)
```

## c)

This part repeats a) and b), with response variable as $log(1+Y)$, where $Y$ is the variable `mktPrice`.

```{r}

bestsubset <- leaps(x = btc_training[, -c(1, 2, 3)], 
      y = log(btc_training[, "mktPrice"] + 1), 
      int = TRUE, 
      method = c("r2"), 
      nbest = 1,
      names = colnames(btc_training[, -c(1, 2, 3)]), df=NROW(x))




bestsubset$which
```

To answer question in 1c, please refer to the table above.

###################################

```{r}
bestCPmodel <- which.min(bestsubset$Cp)
bestsubset$which[13,]

btc.training.for.bestglm <- subset(btc_training, select = names(btc_training) != "Date")
colnames(btc.training.for.bestglm)[1] <- "y"

bestCPmodel <- lm(y ~ mktCap , data = btc.training.for.bestglm)


library(bestglm)

btc.training.logged <- btc.training.for.bestglm
btc.training.logged$y <- log(btc.training.logged$y + 1)


bestmodel.aic <-
    bestglm(Xy = btc.training.logged,
            family = gaussian,
            IC = "AIC",                 
            method = "exhaustive")
kable(ifelse(bestmodel.aic$BestModels[1,]== TRUE, "*", ""))


bestAICmodel <- lm(y ~ mktCap + blkSize + avgBlkSize + medianConfTime + hashRate + dfcty + minerRev + transFee + costPerTrans +uniqueAdd +numTrans +transTotal+ transExcdPplar + trans_exc_chains_longerthan_100 + outputVol + estTransVol, data = btc.training.logged)



bestmodel.bic <-
    bestglm(Xy = btc.training.logged,
            family = gaussian,
            IC = "BIC",                 
            method = "exhaustive")
kable(ifelse(bestmodel.bic$BestModels[1,]== TRUE, "*", ""))

bestBICmodel <- lm(y ~ mktCap + blkSize + medianConfTime  + minerRev + transFee + costPerTrans + transTotal + transExcdPplar + trans_exc_chains_longerthan_100 + estTransVol, data = btc.training.logged)

predictedCP <- predict(bestCPmodel, btc_testing)
predictedAIC <- predict(bestAICmodel, btc_testing)
predictedBIC <- predict(bestBICmodel, btc_testing)

btc.testing.logged <- btc_testing
btc.testing.logged$y <- log(btc.testing.logged$mktPrice + 1)

predErrCP <- mean((predictedCP - btc.testing.logged$mktPrice) ^ 2)
predErrAIC <- mean((predictedAIC - btc.testing.logged$mktPrice) ^ 2)
predErrBIC <- mean((predictedBIC - btc.testing.logged$mktPrice) ^ 2)

result2b <- as.matrix(c(predErrCP, predErrAIC, predErrBIC))
rownames(result2b) <- c("CP", "AIC", "BIC")
colnames(result2b) <- "Prediction Error"
kable(result2b)

```


# Question 2

## Part I.

```{r}
library(MASS)
library(glmnet)

set.seed(200)
N <- 400
P <- 20

Beta <- c(1:5/5, rep(0, P-5))
Beta0 <- 0.5

# genrate X
V <- matrix(0.5, P, P)
diag(V) <- 1

X <- as.matrix(mvrnorm(N, mu = 3*runif(P)-1, Sigma = V))

# create artifical scale of X
X <- sweep(X, 2, 1:10/5, "*")

# genrate Y
y <- Beta0 + X %*% Beta + rnorm(N)

# check OLS
lm(y ~ X)
```

This is the $\beta$ in OLS regression.


```{r}
# now start the Lasso 
# First we scale and center X, and record them.
# Also center y and record it. dont scale it. 
# now since both y and X are centered at 0, we don't need to worry about the intercept anymore. 
# this is because for any beta, X %*% beta will be centered at 0, so no intercept is needed. 
# However, we still need to recover the real intercept term after we are done estimating the beta. 
# The real intercept term can be recovered by using the x_center, x_scale, y2, and the beta parameter you estimated.
# There are other simpler ways to do it too, if you think carefully. 

x_center  <-colMeans(X)
x_scale <- apply(X, 2, sd)
X2 <- scale(X)

bhat = rep(0, ncol(X2)) # initialize it
ymean = mean(y)
y2 = y - ymean

# now start to write functions 
# prepare the soft thresholding function (should be just one line, or a couple of)

soft_th <- function(b, pen)
{
    ifelse((abs(b)-pen)<0, 0, sign(b)*(abs(b)-pen))
}

# initiate lambda. This is one way to do it, the logic is that I set the first lambda as the largetst gradient. 
# if you use this formula, you will need to calculate this for the real data too.

#lambda = exp(seq(log(max(abs(cov(X2, y2)))), log(0.001), length.out = 100))
lambda = glmnet(X, y)$lambda

# you should write the following function which can be called this way 
# LassoFit(X2, y2, mybeta = rep(0, ncol(X2)), mylambda = lambda[10])

LassoFit <- function(myX, myY, mybeta, mylambda, tol = 1e-10, maxitr = 500)
{
    # initia a matrix to record the objective function value
    f = rep(0, maxitr)
    
    for (k in 1:maxitr)
    {
        # compute residual
        r = myY-myX%*%mybeta
        
        # I need to record the residual sum of squares
        f[k] = mean(r*r)
        
        for (j in 1:ncol(myX))
        {
            # add the effect of jth variable back to r 
            # so that the residual is now the residual after fitting all other variables
          r=r + myX[,j]*mybeta[j]
        
            # apply the soft thresholding function to the ols estimate of the jth variable 
        
          mybeta[j]=soft_th(b=sum(myX[,j]*r)/sum((myX[,j]*myX[,j])),pen=mylambda)
            
            # remove the new effect of jth varaible out of r
          r = r - myX[,j]*mybeta[j]
        }
        
        if (k > 10)
        {
            # this is just my adhoc way of stoping rule, you dont have to use it
            if (sum(abs(f[(k-9):k] - mean(f[(k-9):k]))) < tol) break;
        }
    }
    return (mybeta)
}

# you should test your function on a large lambda (penalty) level. 
# this should produce a very spase model. 
# keep in mind that these are not the beta in the original scale of X

LassoFit(X2, y2, mybeta = rep(0, ncol(X2)), mylambda = lambda[10], tol = 1e-7, maxitr = 500)
```



```{r}
# now initiate a matrix that records the fitted beta for each lambda value 

beta_all <- matrix(NA, ncol(X), length(lambda))

# this vecter stores the intercept of each lambda value
beta0_all <- rep(NA, length(lambda))

# this part gets pretty tricky: you will initial a zero vector for bhat, 
# then throw that into the fit function using the largest lambda value. 
# that will return the fitted beta, then use this beta on the next (smaller) lambda value
# iterate until all lambda values are used

bhat <- rep(0, ncol(X2)) # initialize it

for (i in 1:length(lambda)) # loop from the largest lambda value
{
    # if your function is correct, this will run pretty fast
    
    bhat <- LassoFit(X2, y2, bhat, lambda[i])
    
    # this is a tricky part, since your data is scaled, you need to figure out how to scale that back 
    # save the correctly scaled beta into the beta matrix 
    
    beta_all[, i] <- bhat/x_scale 
    
    # here, you need to figure out a way to recalculte the intercept term in the original, uncentered and unscaled X

    beta0_all[i]  <- ymean-mean(apply(X%*%beta_all[,i],1,sum))
}


# now you have the coefficient matrix 
# each column correspond to one lambda value 
#rbind("intercept" = beta0_all, beta_all)


# you should include a similar plot like this in your report
# feel free to make it look better
matplot(colSums(abs(beta_all)), t(beta_all), type="l")
```


```{r}
# The following part provides a way to check your code. 
# You do not need to include this part in your report. 

# However, keep in mind that my original code is based on formula (3)
# if you use other objective functions, it will be different, and the results will not match
 
# load the glmnet package and get their lambda 
library(glmnet)

# this plot should be identical (close) to your previous plot
plot(glmnet(X, y))


# set your lambda to their lambda value and rerun your algorithm 
#lambda = glmnet(X, y)$lambda

# then this distance should be pretty small 
# my code gives distance no more than 0.01
max(abs(beta_all - glmnet(X, y)$beta))
max(abs(beta0_all - glmnet(X, y)$a0))
```


## Part II.

```{r}
X2 <- as.matrix(btc_training[,-which(names(btc_training) %in% c("mktPrice","Date"))])
Y2 <- as.matrix(btc_training[,which(names(btc_training) == "mktPrice")])
#lambda <- seq(20,30,by=0.1)


x_center <- colMeans(X2)
x_scale <- apply(X2, 2, sd)
X3 <- scale(X2)

ymean <- mean(Y2)
Y3 <- Y2 - ymean

lambda <- glmnet(X2, Y2)$lambda

beta_all <- matrix(NA, ncol(X3), length(lambda))

beta0_all <- rep(NA, length(lambda))

bhat <- rep(0, ncol(X3)) # initialize it
for (i in 1:length(lambda))
{
    #bhat <- LassoFit(X3, Y3, mybeta = rep(0, ncol(X3)), mylambda = lambda[i], tol = 1e-7, maxitr = 500)
    bhat <- LassoFit(X3,Y3, bhat, lambda[i])
    beta_all[, i] <- bhat / x_scale 
    beta0_all[i] <- ymean-mean(apply(X2%*%beta_all[,i],1,sum))
}
```


```{r}

Training=btc[1:1460,]
Test=btc[1461:1588,]
Training=Training[,-which(names(Training) == "tradeVol")]
Test=Test[,-which(names(Test) == "tradeVol")]


Test_dataset=as.matrix(Test[,-which(names(Training) %in% c("mktPrice","Date"))])
Test_Y=Test[,which(names(Training) == c("mktPrice"))]
error= matrix(0,length(lambda))
for (i in 1:length(lambda))
{
error[i]=mean((Test_Y-as.vector(beta0_all[i]+Test_dataset%*%matrix(beta_all[,i],ncol=1)))^2)
}

error
lambda[which.min(error)] # This outputs the lowest testing error.

```


```{r}
model=glmnet(X2,Y2)
test_pred=predict(model,as.matrix(Test_dataset),type='response')
error=mean((test_pred-Test[2])^2)
max(abs(beta_all - glmnet(X2, Y2)$beta))
max(abs(beta0_all - glmnet(X2, Y2)$a0))
matplot(colSums(abs(beta_all)), t(beta_all), type="l")
plot(glmnet(X2,Y2))

```

